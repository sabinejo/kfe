{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get data \n",
    "\n",
    "# global variables\n",
    "CC3 = ['KWT', 'BHR', 'OMN', 'QAT', 'SAU', 'ARE', 'YEM', 'ISR', 'PSE', 'JOR', 'LBN', 'SYR',\n",
    "       'EGY', 'IRN', 'TUR', 'IRQ']\n",
    "CC2 = ['KW', 'BH', 'OM', 'QA', 'SA', 'AE', 'YE', 'IL', 'PS', 'JO', 'LB', 'SY', 'EG', 'IR', \n",
    "       'TR', 'IQ']\n",
    "CCS = [690, 692, 698, 694, 670, 696, 680, 666, 'NaN', 663, 660, 652, 651, 630, 640, 645]\n",
    "FIPS = ['KU', 'BA', 'MU', 'QA', 'SA', 'AE', 'YM', 'IS', 'NaN', 'JO', 'LE', 'SY', 'EG', 'IR', \n",
    "        'TU', 'IZ']\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "###\n",
    "# create df from csv and filter by country code column\n",
    "def csv_to_df_CC_filter(filename, country_codes, csv_country_code_column):\n",
    "    df = pd.read_csv(filename , sep=',')\n",
    "    return df[df[csv_country_code_column].isin(country_codes)]\n",
    "\n",
    "# example input and call\n",
    "# UCDP_filename = '/Users/sabine.a.joseph/Downloads/ged50-csv/ged50.csv'\n",
    "# country_code_column_name = 'gwno'\n",
    "# df = csv_to_df_CC_filter(UCDP_filename, CCS, country_code_column_name)\n",
    "\n",
    "###\n",
    "# delete not to be used columns\n",
    "def del_columns_from_df(col_names):\n",
    "    for i in col_names:\n",
    "        del df[i]\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#vars_to_del = ['relid', 'year', 'isocc', 'gwab']\n",
    "#df = del_columns_from_df(vars_to_del)\n",
    "\n",
    "from datetime import datetime\n",
    "###\n",
    "# str date column to datetime index\n",
    "def str_to_datetime(col_name, dateformat):\n",
    "    return [datetime.strptime(str(df[col_name][i]), dateformat) \n",
    "            for i in range(0, len(df[col_name])) if i is not None]\n",
    "\n",
    "# example input and call\n",
    "#df_datestring_column_name = 'date_start'\n",
    "#dateformat = '%Y-%m-%d'\n",
    "#df[df_datestring_column_name] = str_to_datetime(df_datestring_column_name, dateformat)\n",
    "\n",
    "import urllib\n",
    "import lxml.html\n",
    "###\n",
    "# phoenix data: get list of all links off Phoenix website\n",
    "def connect_to_url_get_links(url):\n",
    "    connection = urllib.urlopen(url)\n",
    "    dom =  lxml.html.fromstring(connection.read())\n",
    "\n",
    "    links = []\n",
    "    for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links)\n",
    "        links.append(link) #all download links in list\n",
    "    del links[0:4] #ugly hack\n",
    "    return links\n",
    "\n",
    "# example input and call\n",
    "# url = 'http://phoenixdata.org/data'\n",
    "#links = connect_to_url_get_links(url)\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "###\n",
    "# phoenix data: unzips all phoenix files and returns list of filenames\n",
    "# downloads all files in dir #lengthy!!!\n",
    "def download_and_unzip_files(down_dir, links):\n",
    "    # For every line in the file\n",
    "    for url in links:\n",
    "        # Split on the rightmost / and take everything on the right side of that\n",
    "        name = url.rsplit('/', 1)[-1]\n",
    "\n",
    "        # Combine the name and the downloads directory to get the local filename\n",
    "        filename = os.path.join(down_dir, name)\n",
    "\n",
    "        # Download the file if it does not exist\n",
    "        if not os.path.isfile(filename):\n",
    "            urllib.urlretrieve(url, filename)\n",
    "\n",
    "    # unzip all files\n",
    "    filenames = []\n",
    "    for filename in os.listdir(down_dir):\n",
    "        if filename.endswith(\".txt.zip\"): \n",
    "            filenames.append(filename)\n",
    "            with zipfile.ZipFile(down_dir + '/' + filename) as zip_ref:\n",
    "                zip_ref.extractall(down_dir)\n",
    "    return filenames\n",
    "\n",
    "# example input and call\n",
    "# DOWNLOADS_DIR = '/Users/sabine.a.joseph/Documents/sabine.a.joseph/Documents/Phoenix_event_data'\n",
    "#filenames = download_and_unzip_files(DOWNLOADS_DIR, links)\n",
    "# links as returned by connect_to_url_get_links function\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "###\n",
    "# aggregate per country / bbox and month\n",
    "# index needs to be datetime\n",
    "# enter country_col_name as geo-switch: takes country code or bbox\n",
    "def agg_by_geo_by_month(df, agg_dict, country_col_name):\n",
    "    agg_df = df.groupby([df.index, country_col_name]).agg(aggregations)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    agg_df.columns = agg_df.columns.get_level_values(0)\n",
    "    return agg_df\n",
    "\n",
    "# example input and call\n",
    "#df['count_num_daily_events'] = 1 \n",
    "\n",
    "# create aggregates\n",
    "#aggregations = {\n",
    " #   'protest' : {'protest_events': 'sum'},\n",
    "  #  'material_conflict' : {'material_conflict_events': 'sum'},\n",
    "   # 'rebellion' : {'rebellion_events': 'sum'},\n",
    "    #'GoldsteinScale' : {\n",
    "    #'gs_median': 'median',\n",
    "    #'gs_min': lambda x: min(x),\n",
    "    #'gs_max': lambda x: max(x)},\n",
    "    #'count_num_daily_events' : {'count_num_daily_events': 'sum'}\n",
    "#}\n",
    "\n",
    "# geo-level aggregation switch: country vs grid\n",
    "\n",
    "#agg_df = agg_by_geo_by_month(df, aggregations, 'SourceActorFull') # or 'bbox' for grid level aggregation\n",
    "#agg_df.rename(columns = {list(agg_df)[4]: 'gs_median', \n",
    " #                        list(agg_df)[5]: 'gs_median', \n",
    "  #                       list(agg_df)[6]: 'gs_median'}, inplace = True)\n",
    "\n",
    "###\n",
    "# phoenix data: load all data from individual files to df and filter by for country #lengthy!!!\n",
    "# requires filenames from download_and_unzip_files function\n",
    "def data_to_df(down_dir, col_names, country_codes, filter_col):\n",
    "    for i in range(0, len(filenames)):\n",
    "        if i == 0: #create initial df on first loop iteration\n",
    "            df = pd.read_table(down_dir + '/' + filenames[i][:-4], delim_whitespace=False, \n",
    "                               names=col_names)\n",
    "        else: #concatenate df on each iteration\n",
    "            df = pd.concat([df, pd.read_table(down_dir + '/' + filenames[i][:-4], delim_whitespace=False, \n",
    "                               names = col_names)]) \n",
    "            df = df[df[filter_col].isin(country_codes)] \n",
    "\n",
    "    df = df[df[filter_col].isin(country_codes)]  \n",
    "    df = df.reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#col_names = ('EventID', 'NewsSources')\n",
    "#country_code_filter_col_name = 'SourceActorFull'\n",
    "#df = data_to_df(DOWNLOADS_DIR, col_names, CC3, country_code_filter_col_name)\n",
    "\n",
    "###\n",
    "# save df as csv\n",
    "def df_to_csv(df, path, filename):\n",
    "    df.to_csv(path + filename)\n",
    "\n",
    "# example input and call\n",
    "# path = '/Users/sabine.a.joseph/Documents/Phoenix_event_data/'\n",
    "#csv_name = 'Phoenix_NaMo_subset.csv'\n",
    "#df_to_csv(df, path, csv_name) \n",
    "\n",
    "###\n",
    "# create df from csv\n",
    "def csv_to_df(path, filename):\n",
    "    df = pd.read_csv(path + filename, sep = ',', low_memory=False)\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "# path = '/Users/sabine.a.joseph/Documents/Phoenix_event_data/'\n",
    "#csv_name = 'Phoenix_NaMo_subset.csv'\n",
    "#df = csv_to_df(path, csv_name)\n",
    "\n",
    "###\n",
    "# get and format gridcell data\n",
    "def correct_coordinate_format(df, colname_list):\n",
    "    for i in range(0, len(colname_list)):\n",
    "        df[colname_list[i]] = [(float(df[colname_list[i]][j][:5])) for j in range (0, len(df[colname_list[i]]))]\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#df_grid = pd.read_csv('/Users/sabine.a.joseph/Documents/Desktop/Gridcells_with_countryinfo.csv', sep = ';')\n",
    "#df_grid = correct_coordinate_format(df_grid, ['xmin', 'xmax', 'ymin', 'ymax'])\n",
    "\n",
    "from rtree import index\n",
    "import math\n",
    "###\n",
    "# get bounding box index column\n",
    "def rtree_index_to_bbox_column(df_lon_col, df_lat_col):    \n",
    "    idx = index.Index()\n",
    "    # create rtree index, contains all bounding boxes\n",
    "    for i in range(0, len(df_grid.id)):\n",
    "        # if interleaved is True: xmin, ymin, xmax, ymax\n",
    "        idx.insert(i, (df_grid.xmin[i], df_grid.ymin[i], df_grid.xmax[i], df_grid.ymax[i]))\n",
    "    \n",
    "    # retrieve intersection idx for each coordinate pair\n",
    "    return [(list(idx.intersection((float(df_lon_col[i]), float(df_lat_col[i]), \n",
    "                                    float(df_lon_col[i]), float(df_lat_col[i])))))[0]\n",
    "            if math.isnan(df_lat_col[i]) is False and (list(idx.intersection((float(df_lon_col[i]), float(df_lat_col[i]), \n",
    "                                                                          float(df_lon_col[i]), float(df_lat_col[i])))))\n",
    "            else np.nan for i in range (0, df.shape[0])]\n",
    "\n",
    "# example input and call\n",
    "#df['bbox'] = rtree_index_to_bbox_column(df.longitude, df.latitude)\n",
    "\n",
    "###\n",
    "# url and event ID duplicate removal\n",
    "# create new columns for protest, material conflict, rebellion, radicalism\n",
    "# cast Goldstein to float\n",
    "def EoI_columns(df, col_name_dict):\n",
    "    # max eventid for each url \n",
    "    if col_name_dict['url_name'] and col_name_dict['eventID_name'] is not None: \n",
    "        gdelt_max_id = df.groupby(col_name_dict['url_name'])[col_name_dict['eventID_name']].max()\n",
    "        # keep only max ids to remove duplicates\n",
    "        df = df[df[col_name_dict['eventID_name']].isin(gdelt_max_id)]\n",
    "    if col_name_dict['root_code_name'] is not None: \n",
    "        df['protest'] = np.where(df[col_name_dict['root_code_name']]==14, 1, 0)\n",
    "    if col_name_dict['quad_class_name'] is not None:\n",
    "        df['material_conflict'] = np.where(df[col_name_dict['quad_class_name']]==4, 1, 0)   \n",
    "    if col_name_dict['actor_name'] is not None: \n",
    "        df['rebellion'] = np.where(df[col_name_dict['actor_name']].isin(['REB','SEP','INS']), 1, 0)\n",
    "    if col_name_dict['Actor1Code'] and col_name_dict['Actor2Code'] and col_name_dict['Actor3Code'] is not None: \n",
    "        df['radicalism'] = np.where(np.logical_or.reduce((df[col_name_dict['Actor1Code']]=='RAD',\n",
    "                                                          df[col_name_dict['Actor2Code']]=='RAD',\n",
    "                                                          df[col_name_dict['Actor3Code']]=='RAD')),1, 0)\n",
    "    if 'goldstein_name' in col_name_dict:\n",
    "        df['GoldsteinScale'] = df[col_name_dict['goldstein_name']].apply(lambda x : float(x))\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "# df is event df\n",
    "# Phoenix column names\n",
    "#col_names = {\n",
    " #   'eventID_name' : 'EventID',\n",
    "  #  'root_code_name' : 'EventRootCode',\n",
    "   # 'quad_class_name': 'PentaClass',\n",
    "    #'geo_country_name' : 'SourceActorFull',\n",
    "    #'geo_region_name' : 'region',\n",
    "    #'actor_name' : 'TargetActorRole',\n",
    "    #'url_name' : 'URLs',\n",
    "    #'goldstein_name' : 'GoldsteinScore',\n",
    "    #'date_name' : 'Date',\n",
    "    #'Actor1Code': None,\n",
    "    #'Actor2Code': None,\n",
    "    #'Actor3Code': None\n",
    "#}\n",
    "\n",
    "#df = EoI_columns(df, col_names)\n",
    "\n",
    "###\n",
    "# aggregate per country / bbox and month\n",
    "# index needs to be datetime\n",
    "# enter country_col_name as geo-switch: takes country code or bbox\n",
    "def agg_by_geo_by_month(df, agg_dict, country_col_name):\n",
    "    agg_df = df.groupby([df.index, country_col_name]).agg(agg_dict)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    agg_df.columns = agg_df.columns.get_level_values(0)\n",
    "    return agg_df\n",
    "\n",
    "# example input and call\n",
    "#df['count_num_daily_events'] = 1 \n",
    "\n",
    "# create aggregates\n",
    "#aggregations = {\n",
    " #   'protest' : {'protest_events': 'sum'},\n",
    "  #  'material_conflict' : {'material_conflict': 'sum'},\n",
    "   # 'rebellion' : {'rebellion_events': 'sum'},\n",
    "    #'GoldsteinScale' : {\n",
    "    #'gs_median': 'median',\n",
    "    #'gs_min': lambda x: min(x),\n",
    "    #'gs_max': lambda x: max(x)},\n",
    "    #'AvgTone' : {\n",
    "    #'at_median': 'median',\n",
    "    #'at_min': lambda x: min(x),\n",
    "    #'at_max': lambda x: max(x)},\n",
    "    #'count_num_daily_events' : {'count_num_daily_events': 'sum'},\n",
    "    #'NumMentions' : {'NumMentions': 'sum'},\n",
    "    #'NumSources' : {'NumSources': 'sum'},\n",
    "    #'NumArticles' : {'NumArticles': 'sum'}\n",
    "#}\n",
    "\n",
    "# geo-level aggregation switch: country vs grid\n",
    "\n",
    "#agg_df = agg_by_geo_by_month(df, aggregations, 'ActionGeo_CountryCode') # or 'bbox' for grid level aggregation\n",
    "# rename columns \n",
    "\n",
    "###\n",
    "# create empty df\n",
    "def create_empty_df(date_format, start_date, end_date, CC_col_names, CC_lists, geo_level, date_freq, bbox):\n",
    "\n",
    "    df = pd.DataFrame(index=pd.date_range(start = start_date, end = end_date, freq=date_freq))\n",
    "    df.index = [df.index[i].strftime(new_format) for i in range(0, len(df.index)) if i is not None]\n",
    "    df.reset_index(inplace=True)\n",
    "    df.columns = ['Date']\n",
    "    df = pd.concat([df]*geo_level, ignore_index=True)\n",
    "    df = df.sort_values(by = 'Date')\n",
    "    df.reset_index(inplace=True, drop=True)\n",
    "    df.shape\n",
    "\n",
    "    if geo_level is 16: # country level\n",
    "        for i in range(0, len(CC_lists)):\n",
    "            temp = CC_lists[i] * (len(df.index)/geo_level)\n",
    "            df[CC_col_names[i]] = temp\n",
    "            df.is_copy = False\n",
    "    else: # bbox level \n",
    "        temp = bbox * (len(df.index)/geo_level)\n",
    "        df['bbox'] = temp\n",
    "        df.is_copy = False\n",
    "        \n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#date_format = '%Y-%m-%d'\n",
    "#start_date = '1990-01-01 00:00:00'\n",
    "#end_date = '2017-08-01 00:00:00'\n",
    "#CC_col_names = ['CC3', 'CCS', 'CC2', 'FIPS']\n",
    "#CC_lists = [CC3, CCS, CC2, FIPS] # list of lists of global variables \n",
    "\n",
    "# geo-level switch: country vs. grid (bbox)\n",
    "#geo_level = len(CCS) #len(bbox) \n",
    "\n",
    "# date-freq switch: monthly vs yearly\n",
    "#date_freq = 'MS' #'YS'\n",
    "# df = create_empty_df(date_format, start_date, end_date, CC_col_names, CC_lists, geo_level, date_freq, bbox)\n",
    "\n",
    "###\n",
    "def append_empty_cols_to_df(df, col_names):\n",
    "    for i in col_names:\n",
    "        df[i] = np.nan\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#df = append_empty_cols_to_df(df, cols_to_append_UCDP)\n",
    "#df_UCDP.columns = ['date_start', 'CCS', 'UCDP_low_est']\n",
    "\n",
    "###\n",
    "# combine empty df with event df\n",
    "# call multiple times to combine with all event dfs\n",
    "def combine_empty_and_filled_df(df, df_agg, CC_col_names, CC_lists):\n",
    "    column_names = df_agg.columns\n",
    "    \n",
    "    for i in range(0, df.shape[0]):\n",
    "        match_against = (df_agg.date_start == str(df.date_start[i]))\n",
    "        \n",
    "        if CC_col_names[1] in column_names and df.CC3[i] != 'PSE': # UCDP\n",
    "            df_temp = df_agg[match_against & (df_agg[CC_col_names[1]] == int(df[CC_col_names[1]][i]))] \n",
    "            \n",
    "        elif CC_col_names[0] in column_names and df.CC3[i] != 'PSE': # Phoenix\n",
    "            df_temp = df_agg[match_against & (df_agg[CC_col_names[0]] == df[CC_col_names[0]][i])]\n",
    "            \n",
    "        elif CC_col_names[3] in column_names: # GDELT\n",
    "            df_temp = df_agg[match_against & (df_agg[CC_col_names[3]] == df[CC_col_names[3]][i])]\n",
    "                \n",
    "        if df_temp.empty is False:\n",
    "            for k in column_names:\n",
    "                df.set_value(i, k, df_temp.iloc[0][k] )\n",
    "                df.is_copy = False\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#combined_df = combine_empty_and_filled_df(df, df_UCDP, CC_col_names, CC_lists)\n",
    "#combined_df = combine_empty_and_filled_df(combined_df, df_Phoe, CC_col_names, CC_lists)\n",
    "\n",
    "########################################################\n",
    "###### no longer in use due to performance issue #######\n",
    "import glob\n",
    "###\n",
    "# returns list with all shape files per country \n",
    "def filenames_per_country_list(country_code, path):\n",
    "    folder_name = country_code + '_adm_shp/'\n",
    "    return glob.glob(path_to_shape + folder_name + '/*.shp')\n",
    "\n",
    "# example input and call\n",
    "#path_to_shape = '/Users/sabine.a.joseph/Downloads/' #folders with shapefiles per country\n",
    "#filenames_per_country = filenames_per_country_list(df.SourceActorFull[0], path_to_shape)\n",
    "\n",
    "import fiona\n",
    "import shapely \n",
    "from shapely.geometry import Point, shape\n",
    "from fiona import collection\n",
    "###\n",
    "# looks up given coordinate in a countries` shapefile and assigns value to region column\n",
    "def fill_region_column(df, lon, lat, country_code):\n",
    "    # empty column for region\n",
    "    df['region']=np.nan\n",
    "    \n",
    "    for i in range(0, df.shape[0]):\n",
    "        filenames_per_country = filenames_per_country_list(country_code[i], path_to_shape)\n",
    "\n",
    "        # loop through all regions within country (dep. on N shapefiles for each region)\n",
    "        for k in range(0, len(filenames_per_country)):\n",
    "\n",
    "            with fiona.open(filenames_per_country[k], 'r') as fiona_collection:\n",
    "                shapefile_record = next(iter(fiona_collection))\n",
    "                shape = shapely.geometry.asShape(shapefile_record['geometry'])\n",
    "                point = shapely.geometry.Point(float(lon[i]),\n",
    "                                               float(lat[i])) # longitude, latitude\n",
    "\n",
    "            if shape.contains(point):\n",
    "                df.loc[i, 'region'] = filenames_per_country[k][-12:-4]\n",
    "\n",
    "            fiona_collection.close()\n",
    "    return df\n",
    "\n",
    "# example input and call\n",
    "#df = fill_region_column(df, df.Actor1Geo_Long, df.Actor1Geo_Lat, df.Actor1Code)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
