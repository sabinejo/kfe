{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import lxml.html\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from rtree import index\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read excel with country names and codes\n",
    "# return list with country codes relevant for UCDP\n",
    "country_code_pathfile = '/Users/sabine.a.joseph/Desktop/Country_codes_NAMO.xlsx'\n",
    "country_code_column_name = 'Country_3'\n",
    "sheet = 'Sheet1'\n",
    "\n",
    "def country_codes_from_excel(country_codes, sheet_num, column_name):\n",
    "    countries = pd.ExcelFile(country_codes)\n",
    "    countries = countries.parse(sheet_num)\n",
    "    return list(countries[column_name]) # UCDP uses Gleditsch and Ward country codes\n",
    "\n",
    "CC3 = country_codes_from_excel(country_code_pathfile, sheet, country_code_column_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://phoenixdata.org/data'\n",
    "\n",
    "def connect_to_url_get_links(url):\n",
    "    connection = urllib.urlopen(url)\n",
    "    dom =  lxml.html.fromstring(connection.read())\n",
    "\n",
    "    links = []\n",
    "    for link in dom.xpath('//a/@href'): # select the url in href for all a tags(links)\n",
    "        links.append(link) #all download links in list\n",
    "    del links[0:4] \n",
    "    return links\n",
    "\n",
    "links = connect_to_url_get_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloads all files in dir #lengthy!!!\n",
    "DOWNLOADS_DIR = '/Users/sabine.a.joseph/Documents/sabine.a.joseph/Documents/Phoenix_event_data'\n",
    "\n",
    "def download_and_unzip_files(down_dir, links):\n",
    "    # For every line in the file\n",
    "    for url in links:\n",
    "        # Split on the rightmost / and take everything on the right side of that\n",
    "        name = url.rsplit('/', 1)[-1]\n",
    "\n",
    "        # Combine the name and the downloads directory to get the local filename\n",
    "        filename = os.path.join(down_dir, name)\n",
    "\n",
    "        # Download the file if it does not exist\n",
    "        if not os.path.isfile(filename):\n",
    "            urllib.urlretrieve(url, filename)\n",
    "\n",
    "    # unzip all files\n",
    "    filenames = []\n",
    "    for filename in os.listdir(down_dir):\n",
    "        if filename.endswith(\".txt.zip\"): \n",
    "            filenames.append(filename)\n",
    "            with zipfile.ZipFile(down_dir + '/' + filename) as zip_ref:\n",
    "                zip_ref.extractall(down_dir)\n",
    "    return filenames\n",
    "                \n",
    "filenames = download_and_unzip_files(DOWNLOADS_DIR, links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data to df, for relevant country #lengthy!!!\n",
    "col_names = ('EventID', 'Date', 'Year', 'Month', 'Day', 'SourceActorFull', 'SourceActorEntity', \n",
    "             'SourceActorRole', 'SourceActorAttribute', 'TargetActorFull', 'TargetActorEntity', \n",
    "             'TargetActorRole', 'TargetActorAttribute', 'EventCode', 'EventRootCode', \n",
    "             'PentaClass', 'GoldsteinScore', 'Issues', 'Lat', 'Lon', 'LocationName', \n",
    "             'StateName', 'CountryCode', 'SentenceID', 'URLs', 'NewsSources')\n",
    "country_code_filter_col_name = 'SourceActorFull'\n",
    "\n",
    "def data_to_df(down_dir, col_names, country_codes, filter_col):\n",
    "    for i in range(0, len(filenames)):\n",
    "        if i == 0: #create initial df on first loop iteration\n",
    "            df = pd.read_table(down_dir + '/' + filenames[i][:-4], delim_whitespace=False, \n",
    "                               names=col_names)\n",
    "        else: #concatenate df on each iteration\n",
    "            df = pd.concat([df, pd.read_table(down_dir + '/' + filenames[i][:-4], delim_whitespace=False, \n",
    "                               names = col_names)]) \n",
    "            df = df[df[filter_col].isin(country_codes)] \n",
    "\n",
    "    df = df[df[filter_col].isin(country_codes)]  \n",
    "    df = df.reset_index(drop = True)\n",
    "    return df\n",
    "\n",
    "df = data_to_df(DOWNLOADS_DIR, col_names, CC3, country_code_filter_col_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get and format gridcell data\n",
    "df_grid = pd.read_csv('/Users/sabine.a.joseph/Desktop/Gridcells_with_countryinfo.csv', sep = ';')\n",
    "\n",
    "def correct_coordinate_format(df, colname_list):\n",
    "    for i in range(0, len(colname_list)):\n",
    "        df[colname_list[i]] = [(float(df[colname_list[i]][j][:5])) for j in range (0, len(df[colname_list[i]]))]\n",
    "    return df\n",
    "\n",
    "df_grid = correct_coordinate_format(df_grid, ['xmin', 'xmax', 'ymin', 'ymax'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rtree_index_to_bbox_column(df_lon_col, df_lat_col):    \n",
    "    idx = index.Index()\n",
    "    # create rtree index, contains all bounding boxes\n",
    "    for i in range(0, len(df_grid.id)):\n",
    "        # if interleaved is True: xmin, ymin, xmax, ymax\n",
    "        idx.insert(i, (df_grid.xmin[i], df_grid.ymin[i], df_grid.xmax[i], df_grid.ymax[i]))\n",
    "    \n",
    "    # retrieve intersection idx for each coordinate pair\n",
    "    return [(list(idx.intersection((float(df_lon_col[i]), float(df_lat_col[i]), \n",
    "                                    float(df_lon_col[i]), float(df_lat_col[i])))))[0]\n",
    "            if math.isnan(df_lat_col[i]) is False and (list(idx.intersection((float(df_lon_col[i]), float(df_lat_col[i]), \n",
    "                                                                          float(df_lon_col[i]), float(df_lat_col[i])))))\n",
    "            else np.nan for i in range (0, df.shape[0])]\n",
    "\n",
    "df['bbox'] = rtree_index_to_bbox_column(df.Lon, df.Lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  app.launch_new_instance()\n",
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# url and event ID duplicate removal\n",
    "# create new columns for protest, material conflict, rebellion, radicalism\n",
    "# cast Goldstein to float\n",
    "def EoI_columns(df, col_name_dict):\n",
    "    # max eventid for each url \n",
    "    if col_name_dict['url_name'] and col_name_dict['eventID_name'] is not None: \n",
    "        gdelt_max_id = df.groupby(col_name_dict['url_name'])[col_name_dict['eventID_name']].max()\n",
    "        # keep only max ids to remove duplicates\n",
    "        df = df[df[col_name_dict['eventID_name']].isin(gdelt_max_id)]\n",
    "        #df = df.reset_index()\n",
    "    if col_name_dict['root_code_name'] is not None: \n",
    "        df['protest'] = np.where(df[col_name_dict['root_code_name']]=='14', 1, 0)\n",
    "    if col_name_dict['quad_class_name'] is not None:\n",
    "        df['material_conflict'] = np.where(df[col_name_dict['quad_class_name']]==4, 1, 0)\n",
    "    if col_name_dict['actor_name'] is not None: \n",
    "        df['rebellion'] = np.where(df[col_name_dict['actor_name']].isin(['REB','SEP','INS']), 1, 0)\n",
    "    if col_name_dict['Actor1Code'] and col_name_dict['Actor2Code'] and col_name_dict['Actor3Code'] is not None: \n",
    "        df['radicalism'] = np.where(np.logical_or.reduce((df['Actor1Code']=='RAD',\n",
    "                                                          df['Actor2Code']=='RAD',\n",
    "                                                          df['Actor3Code']=='RAD')),1, 0)\n",
    "    if 'goldstein_name' in col_name_dict:\n",
    "        df['GoldsteinScale'] = df[col_name_dict['goldstein_name']].apply(lambda x : float(x))\n",
    "    return df\n",
    "\n",
    "# Phoenix column names\n",
    "col_names = {\n",
    "    'eventID_name' : 'EventID',\n",
    "    'root_code_name' : 'EventRootCode',\n",
    "    'quad_class_name': 'PentaClass',\n",
    "    'geo_country_name' : 'SourceActorFull',\n",
    "    'geo_region_name' : 'region',\n",
    "    'actor_name' : 'TargetActorRole',\n",
    "    'url_name' : 'URLs',\n",
    "    'goldstein_name' : 'GoldsteinScore',\n",
    "    'date_name' : 'Date',\n",
    "    'Actor1Code': None,\n",
    "    'Actor2Code': None,\n",
    "    'Actor3Code': None\n",
    "}\n",
    "\n",
    "df = EoI_columns(df, col_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save raw df\n",
    "def df_to_csv(df, path, filename):\n",
    "    df.to_csv(path + filename)\n",
    "\n",
    "# example input and call\n",
    "path = '/Users/sabine.a.joseph/Desktop/'\n",
    "csv_name = 'Phoenix_NaMo_subset.csv'\n",
    "df_to_csv(df, path, csv_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path + csv_name, low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date column to datetime index\n",
    "df = df.reset_index(drop=True)\n",
    "df.Date = [str(df.Date[i])[:-2] for i in range (0, len(df.Date)) if i is not None]\n",
    "\n",
    "def str_to_datetime(col_name, dateformat):\n",
    "    return [datetime.strptime(str(df[col_name][i]), dateformat) for i in range(0, df.shape[0]) if i is not None]\n",
    "\n",
    "df_datestring_column_name = 'Date'\n",
    "dateformat = '%Y%m'\n",
    "df[df_datestring_column_name] = str_to_datetime(df_datestring_column_name, dateformat)\n",
    "df.index = df[df_datestring_column_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate per country / bbox and month\n",
    "# index needs to be datetime\n",
    "# enter country_col_name as geo-switch: takes country code or bbox\n",
    "def agg_by_geo_by_month(df, agg_dict, country_col_name):\n",
    "    agg_df = df.groupby([df.index, country_col_name]).agg(agg_dict)\n",
    "    agg_df = agg_df.reset_index()\n",
    "    agg_df.columns = agg_df.columns.get_level_values(0)\n",
    "    return agg_df\n",
    "    \n",
    "df['count_num_daily_events'] = 1 \n",
    "\n",
    "# create aggregates\n",
    "aggregations = {\n",
    "    'protest' : {'protest_events': 'sum'},\n",
    "    'material_conflict' : {'material_conflict_events': 'sum'},\n",
    "    'rebellion' : {'rebellion_events': 'sum'},\n",
    "    'GoldsteinScale' : {\n",
    "    'gs_median': 'median',\n",
    "    'gs_min': lambda x: min(x),\n",
    "    'gs_max': lambda x: max(x)},\n",
    "    'count_num_daily_events' : {'count_num_daily_events': 'sum'}\n",
    "}\n",
    "\n",
    "agg_df = agg_by_geo_by_month(df, aggregations, 'SourceActorFull') # or 'bbox' for grid level aggregation\n",
    "\n",
    "# geo-level aggregation switch: country vs grid\n",
    "country_code = 'SourceActorFull'\n",
    "bbox = 'bbox'\n",
    "\n",
    "agg_df = agg_by_geo_by_month(df, aggregations, country_code) # or 'bbox' for grid level aggregation\n",
    "# rename columns\n",
    "agg_df.columns = ['Date', country_code, 'material_conflict', 'protest', 'gs_median', 'gs_min', \n",
    "                  'gs_max', 'count_num_daily_events', 'rebellion']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save agg df 2x\n",
    "def df_to_csv(df, path, filename):\n",
    "    df.to_csv(path + filename)\n",
    "\n",
    "path = '/Users/sabine.a.joseph/Desktop/'\n",
    "\n",
    "if bbox in agg_df.columns:\n",
    "    csv_name = 'Phoenix_NaMo_agg_subset_BBOX.csv'\n",
    "else:\n",
    "    csv_name = 'Phoenix_NaMo_agg_subset_COUNTRY.csv'\n",
    "\n",
    "df_to_csv(agg_df, path, csv_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
